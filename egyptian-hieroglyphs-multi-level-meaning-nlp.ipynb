{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5362908,"sourceType":"datasetVersion","datasetId":2983655}],"dockerImageVersionId":30407,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Figurative thinking","metadata":{}},{"cell_type":"markdown","source":"In our material and pragmatic world, during the ages we have highly succeeded developing logical thinking. To this date this knowledge expressed in structured descriptive language models like GPT. Although they can demonstrate reasoning, they are not capable to discover the new concepts and go beyond logical interpretation.\n\nAs a human, one of our main functions in to receive, transform and transmit information. The language, first of all is the way of transmitting the information. We want the language to to transmit as much as information per symbol/word/sentence as possible. \n\nLanguage - is more complex concept that we used to think. To understand it completely we have to re-discover our Figurative thinking. Think about this way: logic is our left brain's job, images (figures) - our right brain's job. These halves of the brain work synchronously and complement each other in the process of processing and transmitting information. Remember Inn-Yann concept and symbol? This is the good example when the single symbol can hold the whole concept in it.\nhttps://www.healthline.com/health/left-brain-vs-right-brain\n\nToday we don't fully understand our language, and we don't understand the symbols. We are trying to fing the paths where the word came from: latin, sanskrit or shumeric. Fe fight and devide our world by languages, religions, academia approach, etc. To my mind - the problem in modern languages is that the core meaning of the words is lost due to the highly skewed collective consciousness into the left half brain thinking.\n\nThe key in reading of symbolic texts is in understanding the meaning of symbol-charachter (image, symbol, syllable, sound) and it's interaction with the object this symbol describes. These characters are universal, have the same meaning across the globe and sounds accordingly. \n\nI invite you to this journey and you will discover a new forgotten knowledge. In order to dive into understanding the Structural linguistics concept, try to forget everything you have been taught in life and start to perceive words and symbols like a 3-5yo child. You don't know english, french or russian written language. You know letters, numbers and can read the symbols. \n\nHow do we code in symbols 2 very basic concepts for our child: laugh and wonder for example? These concepts are universal and don't depend on language. The language depend on them. When we laugh we pronounce \"HA\". When we wonder we pronounce \"AH\". Now try to draw a simple symbol to hold these concept. Reversing the symbol should flip the concept. Did you get the idea? Thins is a very basic example of how early Egyptians coded the Universal knowledge in symbols using Structural linguistics.\n\nI have started this project inspired by works of russian speaking egyptologist, symbols expert and language researcher - https://www.youtube.com/@Ur_Al\n\nHe brings very interesting ideas of interpretting egyptian texts using old Russian, Sanskrit and Latin language structures, based on the concept of Structural linguistics.\n\nThe main problem is that we do not understand complex multi-level meaning of egyptian texts. They contain much more information then what we see translated today. Rosetta stone origin has many questions and doesn't open us the secrects of hieroglyphs decomposition. \n\nIdeograms (symbols) and phonogram (pronounciation) should match and reflect the one character. \n\nFor example:\n* ANT (very old) - antique, gigant, atlantic, anthem, antropology, infant\n* AR (stopping and holding) - dark, arret, array, arrest, arrow, arc \n* AV (gather goods by travelling) - travel, aviation, navy, slavery, have, save, avoir, caravan \n* BIR, BER, BOR (take, grab, get toghether) - sober, borrowed, berry, cyber, neighbor, berth, graber\n* BRI, BRA (give smth to establish connection) - bridge, brick, bring, bribe, bride, grab, gravel\n* CAR, CYR (current state, curly borders) - circle, circus, circumstance, curly, church, cyrillic, current, curve, cursor, curb, car, cart\n* CRE, CRA, CRI, KRA, KRE, KRI (mounted on top, superior, added) - credit, increase, crawl, crest, chist, creme, crete, concrete, crab, crater, create, cristal\n* CUT, CAT, KAT (categorise, separate, specialise, cut) - cut, category, catastrophe, catalogue, education, cation, cathode, cathedra\n\netc...\n\nI will go further Saussure's concepts and try to create the Universal Language model in order to decode ancient languages and helps to remember the root meaning of the words in modern languages.\n","metadata":{}},{"cell_type":"markdown","source":"# Semiotics","metadata":{}},{"cell_type":"markdown","source":"Constructed languages","metadata":{}},{"cell_type":"markdown","source":"# Structural Linguistics","metadata":{}},{"cell_type":"markdown","source":"In Structural linguistics (Structuralism) we don't look for the causes or origins of language (or of any other phenomenon). Structuralism looks for the rules that underlie language and govern how it functions: it looks for the structure.\n\nThe components of a word structure are not merely a collection of independent items: they form a working unit because they exist in relation to one another. They interact and have the sequence importance. This is the science where Computer Vision and NLP work together for one purpose.\n\nWords do not simply refer to objects in the world for which they stand. Instead, a word is a linguistic sign consisting, like the two sides of a coin, of two inseparable parts: signifier + signified. A signifier is a “sound-image” (a mental imprint of a linguistic sound or hieroglyph); the signified is the concept to which the signifier refers. Thus, a word is not merely a sound-image (signifier), nor is it merely a concept (signified). A sound image becomes a word only when it is linked with a concept. \n\nThe idea that signifiers, or linguistic sound-images, do not refer to things in the world but to concepts in our mind is crucial for structuralism. \n\nhttps://literariness.org/2018/12/22/structural-linguistics/\n\nhttps://en.wikipedia.org/wiki/Structural_linguistics","metadata":{}},{"cell_type":"markdown","source":"# Universal language concept","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Alphabeth of thoughts (Slo-grams)","metadata":{}},{"cell_type":"markdown","source":"Morphemes/Syllables/Symbols connection. I am sharing with you the Slovo-grams dictionary (proto-syllables), english and russian words containing them, and the core meaning of this syllable. These are the basic buidlding blocks of most Indo-European group of languages. Some languages preserved more of this structure, other languages lost the meaning of the words through the ages. Using this approach I will try to discover the hidden connection between the characters and their meanings no matter the language we use.\n\nThe data for Slovo-gram dictionary is taken troughout the years by me from:\n\n1. Other researcher's work (list will be provided)\n2. Knowledge received learning Egyptian texts\n3. Cross-translation techniques\n4. Common sense\n\nThe Work is still in progress.\n\nSurprisingly 500 slo-grams correspond to 500 Egyptian symbols","metadata":{}},{"cell_type":"code","source":"# importing slo-grams into the df\nimport pandas as pd\nfrom IPython.display import display\n\n\ndf = pd.read_excel(\"/kaggle/input/slovo-grams/Slovo_Grams.xlsx\")\n\ndf.head(50)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:52:55.272731Z","iopub.execute_input":"2023-04-10T13:52:55.273219Z","iopub.status.idle":"2023-04-10T13:52:55.506713Z","shell.execute_reply.started":"2023-04-10T13:52:55.273182Z","shell.execute_reply":"2023-04-10T13:52:55.505685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:37:54.557300Z","iopub.execute_input":"2023-04-10T13:37:54.557714Z","iopub.status.idle":"2023-04-10T13:37:54.566060Z","shell.execute_reply.started":"2023-04-10T13:37:54.557681Z","shell.execute_reply":"2023-04-10T13:37:54.564709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Root Meaning Tokenization","metadata":{}},{"cell_type":"code","source":"# searching for syllables in the slogram column and return their meaning value\n\nimport pandas as pd\n\ndef meaning(word):\n    df[\"slogram\"] = df[\"slogram\"].astype(str)\n    df[\"meaning\"] = df[\"meaning\"].astype(str)\n    slo_grams = list(df[\"slogram\"].str.lower())\n    meanings = list(df[\"meaning\"])\n    tokens = []\n    i = 0\n    while i < len(word):\n        for j, s in enumerate(slo_grams):\n            if word[i:].startswith(s):\n                tokens.append(meanings[j])\n                i += len(s)\n                break\n        else:\n            tokens.append(word[i])\n            i += 1\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:03:08.082040Z","iopub.execute_input":"2023-04-10T13:03:08.082558Z","iopub.status.idle":"2023-04-10T13:03:08.091743Z","shell.execute_reply.started":"2023-04-10T13:03:08.082518Z","shell.execute_reply":"2023-04-10T13:03:08.090465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nmeaning(\"carburator\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:38:02.900535Z","iopub.execute_input":"2023-04-10T13:38:02.900976Z","iopub.status.idle":"2023-04-10T13:38:02.912541Z","shell.execute_reply.started":"2023-04-10T13:38:02.900940Z","shell.execute_reply":"2023-04-10T13:38:02.911117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# root_meaning function that take also values from \"related\" columns also if there is no match \n# also, I modified the function so it looks for 3-symbol syllable first, then looks for 2-symbols match.\n\ndef root_meaning(word):\n    df[\"slogram\"] = df[\"slogram\"].astype(str)\n    df[\"rel_1\"] = df[\"rel_1\"].astype(str)\n    df[\"rel_2\"] = df[\"rel_2\"].astype(str)\n    df[\"meaning\"] = df[\"meaning\"].astype(str)\n    slo_grams = []\n    meanings = []\n    for slogram, rel_1, rel_2, meaning in zip(df[\"slogram\"], df[\"rel_1\"], df[\"rel_2\"], df[\"meaning\"]):\n        if slogram != \"nan\":\n            slo_grams.append(slogram.lower())\n            meanings.append(meaning)\n        if rel_1 != \"nan\":\n            slo_grams.append(rel_1.lower())\n            meanings.append(meaning)\n        if rel_2 != \"nan\":\n            slo_grams.append(rel_2.lower())\n            meanings.append(meaning)\n    tokens = []\n    i = 0\n    while i < len(word):\n        found_match = False\n        for j, s in enumerate(slo_grams):\n            if len(s) == 3 and word[i:].startswith(s):\n                tokens.append(meanings[j])\n                i += len(s)\n                found_match = True\n                break\n        if not found_match:\n            for j, s in enumerate(slo_grams):\n                if len(s) == 2 and word[i:].startswith(s):\n                    tokens.append(meanings[j])\n                    i += len(s)\n                    found_match = True\n                    break\n        if not found_match:\n            tokens.append(word[i])\n            i += 1\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:53:00.887196Z","iopub.execute_input":"2023-04-10T13:53:00.888881Z","iopub.status.idle":"2023-04-10T13:53:00.902294Z","shell.execute_reply.started":"2023-04-10T13:53:00.888796Z","shell.execute_reply":"2023-04-10T13:53:00.900669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"carburator\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:53:04.331232Z","iopub.execute_input":"2023-04-10T13:53:04.331726Z","iopub.status.idle":"2023-04-10T13:53:04.344604Z","shell.execute_reply.started":"2023-04-10T13:53:04.331683Z","shell.execute_reply":"2023-04-10T13:53:04.342739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"cosmos\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:53:08.699549Z","iopub.execute_input":"2023-04-10T13:53:08.700954Z","iopub.status.idle":"2023-04-10T13:53:08.711887Z","shell.execute_reply.started":"2023-04-10T13:53:08.700900Z","shell.execute_reply":"2023-04-10T13:53:08.709973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"creator\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:53:13.088881Z","iopub.execute_input":"2023-04-10T13:53:13.089618Z","iopub.status.idle":"2023-04-10T13:53:13.100547Z","shell.execute_reply.started":"2023-04-10T13:53:13.089577Z","shell.execute_reply":"2023-04-10T13:53:13.098896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"germes\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:53:15.569704Z","iopub.execute_input":"2023-04-10T13:53:15.570935Z","iopub.status.idle":"2023-04-10T13:53:15.580779Z","shell.execute_reply.started":"2023-04-10T13:53:15.570886Z","shell.execute_reply":"2023-04-10T13:53:15.579460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"architecture\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:53:21.109721Z","iopub.execute_input":"2023-04-10T13:53:21.110179Z","iopub.status.idle":"2023-04-10T13:53:21.123271Z","shell.execute_reply.started":"2023-04-10T13:53:21.110134Z","shell.execute_reply":"2023-04-10T13:53:21.121849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"manager\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:53:23.735128Z","iopub.execute_input":"2023-04-10T13:53:23.735701Z","iopub.status.idle":"2023-04-10T13:53:23.749700Z","shell.execute_reply.started":"2023-04-10T13:53:23.735661Z","shell.execute_reply":"2023-04-10T13:53:23.747389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"scanner\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:53:47.232363Z","iopub.execute_input":"2023-04-10T13:53:47.232783Z","iopub.status.idle":"2023-04-10T13:53:47.244017Z","shell.execute_reply.started":"2023-04-10T13:53:47.232749Z","shell.execute_reply":"2023-04-10T13:53:47.242397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"pira-mida\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:54:46.000509Z","iopub.execute_input":"2023-04-10T13:54:46.000940Z","iopub.status.idle":"2023-04-10T13:54:46.012093Z","shell.execute_reply.started":"2023-04-10T13:54:46.000905Z","shell.execute_reply":"2023-04-10T13:54:46.010634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing our root_meaning function\nroot_meaning(\"intuition\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:56:57.487156Z","iopub.execute_input":"2023-04-10T13:56:57.487584Z","iopub.status.idle":"2023-04-10T13:56:57.501177Z","shell.execute_reply.started":"2023-04-10T13:56:57.487548Z","shell.execute_reply":"2023-04-10T13:56:57.499577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, when we know how to deconstruct the words to their root syllables (slograms), we can start working with hieroglyphs to unpack The GERMETIC knowledge!\n","metadata":{}},{"cell_type":"markdown","source":"# Exploring Egyptian hieroglyphs dataset","metadata":{}},{"cell_type":"markdown","source":"This dataset is build from the hieroglyphs found in 10 different pictures from the book \"The Pyramid of Unas\" (Alexandre Piankoff, 1955). \n\nEach hieroglyph is manually annotated and labelled according the Gardiner Sign List. The images are stored with their label and number in their name.\n\nhttps://archive.org/details/pyramidofunas0005unse","metadata":{}},{"cell_type":"code","source":"#!pip install datasets\n#!pip install transformens","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:03:08.191704Z","iopub.execute_input":"2023-04-10T13:03:08.192110Z","iopub.status.idle":"2023-04-10T13:03:08.201344Z","shell.execute_reply.started":"2023-04-10T13:03:08.192067Z","shell.execute_reply":"2023-04-10T13:03:08.200339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!git clone https://huggingface.co/datasets/HamdiJr/Egyptian_hieroglyphs","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:03:08.203120Z","iopub.execute_input":"2023-04-10T13:03:08.203576Z","iopub.status.idle":"2023-04-10T13:03:08.213042Z","shell.execute_reply.started":"2023-04-10T13:03:08.203528Z","shell.execute_reply":"2023-04-10T13:03:08.211840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing the Hugging Face Egyptian hieroglyphs dataset from Hugging Face\nfrom datasets import load_dataset\n\n# Load the dataset using the Hugging Face Datasets library\ndataset = load_dataset(\"HamdiJr/Egyptian_hieroglyphs\");\n\n# Print information about the available splits for the dataset\nprint(\"Available splits:\", list(dataset.keys()))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-10T13:03:08.214674Z","iopub.execute_input":"2023-04-10T13:03:08.215065Z","iopub.status.idle":"2023-04-10T13:07:42.363118Z","shell.execute_reply.started":"2023-04-10T13:03:08.215030Z","shell.execute_reply":"2023-04-10T13:07:42.361552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# displaying 20 random hieroglyphs\n\nimport random\nimport matplotlib.pyplot as plt\n\n# Choose 20 random examples from the train split of the dataset\ntrain_examples = random.sample(list(dataset[\"train\"]), 20)\n\n# Create a 5x4 grid of subplots\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(16, 16))\n\n# Loop through the examples and display each one in a subplot\nfor i, example in enumerate(train_examples):\n    row = i // 5\n    col = i % 5\n    ax = axes[row][col]\n    ax.imshow(example[\"image\"])\n    ax.set_title(example[\"label\"])\n    ax.axis(\"off\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:07:42.365235Z","iopub.execute_input":"2023-04-10T13:07:42.365873Z","iopub.status.idle":"2023-04-10T13:07:44.750679Z","shell.execute_reply.started":"2023-04-10T13:07:42.365828Z","shell.execute_reply":"2023-04-10T13:07:44.749712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the dataset's features\nprint(dataset[\"train\"].features)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:07:44.751675Z","iopub.execute_input":"2023-04-10T13:07:44.752017Z","iopub.status.idle":"2023-04-10T13:07:44.759402Z","shell.execute_reply.started":"2023-04-10T13:07:44.751984Z","shell.execute_reply":"2023-04-10T13:07:44.757904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nsubset = dataset[\"train\"].filter(lambda example: example[\"label\"] == 11)\n\n# Display all the images from subclass\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(16, 16))\n\nfor idx, example in enumerate(subset):\n    row = idx // 5\n    col = idx % 5\n    axes[row, col].imshow(example[\"image\"])\n    axes[row, col].axis(\"off\")\n\n# If there are fewer than 20 images, hide the remaining subplots\nfor idx in range(len(subset), 20):\n    row = idx // 5\n    col = idx % 5\n    axes[row, col].axis(\"off\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:07:44.767480Z","iopub.execute_input":"2023-04-10T13:07:44.768567Z","iopub.status.idle":"2023-04-10T13:07:46.443674Z","shell.execute_reply.started":"2023-04-10T13:07:44.768508Z","shell.execute_reply":"2023-04-10T13:07:46.442177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This symbol sounds like \"CHE\" and represents change in either upside or downside.\n\n","metadata":{}},{"cell_type":"code","source":"# let's check the classes with the most images in it\n\nfrom collections import Counter\n\n# Count the number of images per class\nclass_counts = Counter(example[\"label\"] for example in dataset[\"train\"])\n\n# Get the top 10 classes with the most images\ntop_10_classes = class_counts.most_common(10)\n\n# Print the top 10 classes and their counts\nprint(\"Top 10 classes with the most images:\")\nfor class_id, count in top_10_classes:\n    print(f\"Class {class_id}: {count} images\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:07:46.445212Z","iopub.execute_input":"2023-04-10T13:07:46.445859Z","iopub.status.idle":"2023-04-10T13:07:47.760882Z","shell.execute_reply.started":"2023-04-10T13:07:46.445798Z","shell.execute_reply":"2023-04-10T13:07:47.759987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get the first image from each of the first 20 classes\nfirst_images = []\nfor class_id in range(20):\n    first_image = dataset[\"train\"].filter(lambda example: example[\"label\"] == class_id)[0][\"image\"]\n    first_images.append(first_image)\n\n# Display the first images from the first 20 classes in a 5x4 grid\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(16, 16))\n\nfor idx, image in enumerate(first_images):\n    row = idx // 5\n    col = idx % 5\n    axes[row, col].imshow(image)\n    axes[row, col].axis(\"off\")\n\nplt.show()\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-10T13:07:47.762144Z","iopub.execute_input":"2023-04-10T13:07:47.763026Z","iopub.status.idle":"2023-04-10T13:08:05.983540Z","shell.execute_reply.started":"2023-04-10T13:07:47.762988Z","shell.execute_reply":"2023-04-10T13:08:05.981972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's export first image from first 20 categories to excel spreadsheet for manual annotation \n\nimport numpy as np\nimport os\nfrom io import BytesIO\nfrom PIL import Image\nfrom openpyxl import Workbook\nfrom openpyxl.drawing.image import Image as XLImage\n\n# Define the number of classes you want to take into account\nnum_classes = 20\n\n# Find the first image from each class\nimage_examples = []\nfor class_id in range(num_classes):\n    example = next(iter(dataset[\"train\"].filter(lambda example: example[\"label\"] == class_id)))\n    image_examples.append(example)\n\n# Create a new workbook and add a worksheet\nwb = Workbook()\nws = wb.active\nws.title = \"Image Examples\"\n\n# Resize images, save them temporarily and insert them into the Excel sheet\nthumbnail_size = (50, 50)\nfor idx, example in enumerate(image_examples):\n    img = example[\"image\"]\n    img.thumbnail(thumbnail_size)\n\n    # Convert the image to a NumPy array\n    img_array = np.array(img)\n\n    # Convert the NumPy array back to an image\n    img = Image.fromarray(img_array, mode='L')\n\n    # Save the image to a buffer\n    buffer = BytesIO()\n    img.save(buffer, format=\"PNG\")\n    buffer.seek(0)\n\n    # Insert the image into the worksheet\n    ws.column_dimensions['A'].width = img.width // 6\n    ws.row_dimensions[idx + 1].height = img.height\n    ws.add_image(XLImage(buffer), f\"A{idx + 1}\")\n\n# Save the workbook\nwb.save(\"image_examples.xlsx\")\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-10T13:08:05.985487Z","iopub.execute_input":"2023-04-10T13:08:05.985919Z","iopub.status.idle":"2023-04-10T13:08:23.168417Z","shell.execute_reply.started":"2023-04-10T13:08:05.985882Z","shell.execute_reply":"2023-04-10T13:08:23.167191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# amazing, now, let's take 3 random images from each category and their class label\n\nimport os\nfrom openpyxl import Workbook\nfrom openpyxl.drawing.image import Image as XLImage\nimport random\n\n\n# Define the number of classes you want to take into account\nnum_classes = dataset[\"train\"].features[\"label\"].num_classes\nnum_random = 3\n\n# Create a new workbook and add a worksheet\nwb = Workbook()\nws = wb.active\nws.title = \"Image Examples\"\n\n# Write the header row\nheader = [\"Class\"] + [\"Random Image \" + str(i+1) for i in range(num_random)]\nws.append(header)\n\n# Resize images, save them temporarily and insert them into the Excel sheet\nthumbnail_size = (100, 100)\nfor class_id in range(num_classes):\n    # Find 3 random images from the class\n    examples = dataset[\"train\"].filter(lambda example: example[\"label\"] == class_id)\n    examples = list(examples)\n    random_examples = random.sample(examples, min(num_random, len(examples)))\n\n    # Convert images to thumbnails and save them temporarily\n    image_files = []\n    for i, example in enumerate(random_examples):\n        img = example[\"image\"]\n        img.thumbnail(thumbnail_size)\n\n        # Convert the image to a NumPy array\n        img_array = np.array(img)\n\n        # Convert the NumPy array back to an image\n        img = Image.fromarray(img_array, mode='L')\n\n        # Save the image to a buffer\n        buffer = BytesIO()\n        img.save(buffer, format=\"PNG\")\n        buffer.seek(0)\n\n        # Insert the image into the worksheet\n        xl_img = XLImage(buffer)\n        ws.column_dimensions[chr(66+i)].width = img.width // 6\n        ws.row_dimensions[class_id+2].height = img.height\n        ws.add_image(xl_img, f\"{chr(66+i)}{class_id+2}\")\n\n        # Add the image to the list of image files\n        image_files.append(buffer)\n\n    # Add the class label and image file names to the worksheet\n    row = [dataset[\"train\"].features[\"label\"].int2str(class_id)] + [None]*num_random\n    for i, example in enumerate(random_examples):\n        row[i+1] = dataset[\"train\"].features[\"label\"].int2str(example[\"label\"])\n    ws.append(row)\n\n# Save the workbook\nwb.save(\"image_examples.xlsx\")\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-10T13:08:23.170476Z","iopub.execute_input":"2023-04-10T13:08:23.170964Z","iopub.status.idle":"2023-04-10T13:10:52.887307Z","shell.execute_reply.started":"2023-04-10T13:08:23.170913Z","shell.execute_reply":"2023-04-10T13:10:52.886016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decoding the main badge of \"Pyramid of Unas\"","metadata":{}},{"cell_type":"markdown","source":"Ok, good job. Now we have the exchel spreadsheet with symbol samples from this dataset and their corresponding class (Gardiner's Sign List). Now I can start manually labeling this dataset with my Slovo-gram dictionary.\n\nThis dataset contains only 171 symbol. Some experts estimate Ancient Egypt texts contain up to 700 symbols. My Slovo-gram vocabulary contains around 500 symbols by now. JSesh sowtware that we will also use contain 1500 symbols.\n\nThe annotation (labeling) in the initial dataset is useless for my understanding. We cannot take Egyptian hieroglyphs as letters or words similar to our alphabet. Rather as structure of Symbol-Sound_Action. Symbol is a character and a simplest sound (syllable) at the same time. \n\nThe whole point of this project to create proper annotations to symbols and develop a model on that.\n\nThe proof that I am going in right direction will be the \"Pyramid of Unas\" main lodo decoded with this method. \n\nHere are some images:","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/slovo-grams/Pyramid_of_Unas2.png')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:10:52.889002Z","iopub.execute_input":"2023-04-10T13:10:52.889803Z","iopub.status.idle":"2023-04-10T13:10:52.928261Z","shell.execute_reply.started":"2023-04-10T13:10:52.889753Z","shell.execute_reply":"2023-04-10T13:10:52.927092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, so first of all, Pyramid of Unas = Pyramide of One.\n\nIn this pyramide the concept of wholeness and rules of being Whole (Universal rules) are encoded.\n\nThe main badge contain 5 hieroglyphs:\n1. Rabbit - SCO - scope\n2. Fence - CO - collective\n3. Feather-R - CR - highest\n4. Arc-L - DU - thoughts\n5. O-shaped stand, containing 4 symbols above - NEI - in it\n\nSCO-CO CR(a)DU (v)NEI - Scope of collective highest thoughts here.\n\nPretty meaningfull, isnt't it? You can verify it yourself.\n\nIsn't is that easter hare? \n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/slovo-grams/Pyramid_of_Unas1.png')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:10:52.929656Z","iopub.execute_input":"2023-04-10T13:10:52.930014Z","iopub.status.idle":"2023-04-10T13:10:52.963122Z","shell.execute_reply.started":"2023-04-10T13:10:52.929980Z","shell.execute_reply":"2023-04-10T13:10:52.962194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the example of translation from the Author, first page. Try to find some meaning there.\n\nYou will not find specific coherent, much-less technical information in this translation. There is some poetic meaning. The author of the book \"The Pyramid of Unas\", Alexandre Piankoff, was not translating the main badge, but just writing \"Unas\". \n\nI simply cannot beleive that Egyptians or whoever built that did such a enormous amounts of work in stone just to praise the gods in poems. \n\nI am pretty sure those texts contain almost technical and exact information and instructions.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/slovo-grams/Pyramid_of_Unas3.png')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:10:52.964341Z","iopub.execute_input":"2023-04-10T13:10:52.965455Z","iopub.status.idle":"2023-04-10T13:10:53.002092Z","shell.execute_reply.started":"2023-04-10T13:10:52.965416Z","shell.execute_reply":"2023-04-10T13:10:53.000936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/slovo-grams/Pyramid_of_Unas4.png')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:10:53.003663Z","iopub.execute_input":"2023-04-10T13:10:53.004339Z","iopub.status.idle":"2023-04-10T13:10:53.073890Z","shell.execute_reply.started":"2023-04-10T13:10:53.004299Z","shell.execute_reply":"2023-04-10T13:10:53.072140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Labeling of Egyptian Hierogliphs","metadata":{}},{"cell_type":"markdown","source":"To label properly the Egyptian Hierogliphs dataset with corresponding syllables (sounds) I will you the work of Andrei Bannikov and go further using his approach. https://www.youtube.com/@Ur_Al\n\nThis method of assigning the sound (syllable, slogram) to symbol is very organic, makes you understand the meaning of the symbol (hieroglyph) by looking on it and hearing the corrwsponding sound.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/slovo-grams/Proper_Hieroglyphs_Sounds1.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:10:53.075990Z","iopub.execute_input":"2023-04-10T13:10:53.076373Z","iopub.status.idle":"2023-04-10T13:10:53.126605Z","shell.execute_reply.started":"2023-04-10T13:10:53.076338Z","shell.execute_reply":"2023-04-10T13:10:53.125609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/slovo-grams/Proper_Hieroglyphs_Sounds2.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:10:53.128311Z","iopub.execute_input":"2023-04-10T13:10:53.128958Z","iopub.status.idle":"2023-04-10T13:10:53.181778Z","shell.execute_reply.started":"2023-04-10T13:10:53.128919Z","shell.execute_reply":"2023-04-10T13:10:53.180306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/slovo-grams/Proper_Hieroglyphs_Sounds3.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T13:10:53.183388Z","iopub.execute_input":"2023-04-10T13:10:53.183870Z","iopub.status.idle":"2023-04-10T13:10:53.231391Z","shell.execute_reply.started":"2023-04-10T13:10:53.183831Z","shell.execute_reply":"2023-04-10T13:10:53.230297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Steps of the project","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RAh MAT - Thank you! - Благо Дарю!\n\nAI Human Intelligence coming soon ...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}